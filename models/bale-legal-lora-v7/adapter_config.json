{
"adapter_path": "models/bale-legal-lora-v7",
"batch_size": 1,
"config": null,
"data": "data/mlx_training_v7",
"fine_tune_type": "lora",
"grad_accumulation_steps": 1,
"grad_checkpoint": true,
"iters": 600,
"learning_rate": 5e-06,
"lora_parameters": {
"rank": 8,
"dropout": 0.0,
"scale": 20.0
},
"lr_schedule": null,
"mask_prompt": false,
"max_seq_length": 2048,
"model": "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
"num_layers": 16,
"optimizer": "adam",
"optimizer_config": {
"adam": {},
"adamw": {},
"muon": {},
"sgd": {},
"adafactor": {}
},
"project_name": null,
"report_to": null,
"resume_adapter_file": "models/bale-legal-lora-v6/adapters.safetensors",
"save_every": 150,
"seed": 0,
"steps_per_eval": 100,
"steps_per_report": 50,
"test": false,
"test_batches": 500,
"train": true,
"val_batches": 10
}