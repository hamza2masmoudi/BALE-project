# BALE: A Neuro-Symbolic Framework for Bilingual Contract Risk Assessment
**Authors:** Hamza Masmoudi **Date:** January 2026
## Abstract
Contract review is a critical but labor-intensive legal task, requiring the identification of subtle risks across lengthy documents. While Large Language Models (LLMs) have shown promise in legal text processing, they often struggle with the "stochastic justice" problem—inconsistent reasoning and hallucinated risks—and lack domain-specific grounding in multi-jurisdictional contexts. We present **BALE (Binary Adjudication & Litigation Engine)**, a neuro-symbolic framework that combines fine-tuned LLMs for semantic interpretation with meaningful symbolic logic for deterministic risk scoring. BALE 2.1 introduces a bilingual architecture capable of analyzing commercial contracts in both English (Common Law) and French (Civil Law). We contribute a manually curated "Golden Set" of 91 contract clauses with ground-truth risk assessments. Our experiments show that BALE achieves **98.9% accuracy** in clause type classification and provides transparent, auditible risk scores, offering a practical path toward reliable AI-assisted legal review.
## 1. Introduction
The interpretation of legal contracts is fundamentally a task of logic applied to language. A "high-risk" indemnity clause is not risky because of the specific words used, but because those words trigger specific legal liabilities defined by statute or precedent. Traditional NLP approaches treated this as a classification problem, while modern Generative AI approaches treat it as a reading comprehension task. Both suffer from significant limitations: classification models lack nuance, while generative models lack consistency.
We identify three core challenges in automating contract review:
1. **The "Stochastic Justice" Problem:** LLMs are probabilistic. A model that flags a clause as "high risk" with 80% confidence on Monday and 40% on Tuesday is useless for legal work, where consistency is paramount.
2. **Jurisdictional Blindness:** Most legal LLMs are trained predominantly on US Case Law. They apply American standards of "reasonableness" to French Civil Code contracts, leading to legally invalid advice.
3. **Auditability:** A black-box neural network cannot explain *why* a clause is risky in a way that holds up in court. "The model said so" is not a legal argument.
To address these, we propose **BALE**, a neuro-symbolic system. BALE assigns the "System 1" (Fast, Intuitive) tasks of reading and interpreting text to a fine-tuned LLM, and the "System 2" (Slow, Deliberate) task of final adjudication to a deterministic symbol engine. This hybrid approach ensures that while the parsing of text benefits from the flexibility of neural networks, the final risk score is a strict function of identified legal defects.
## 2. Related Work
**Legal Judgment Prediction:** Early work focused on predicting court outcomes using SVMs and later BERT-based models (Chalkidis et al., 2019). These systems are distinct from *risk assessment*, which is a pre-litigation task.
**Machine Reading on Contracts:** The CUAD dataset (Hendrycks et al., 2021) established a benchmark for clause extraction. BALE goes beyond extraction to *evaluation*, assessing not just "where is the indemnity clause?" but "is this indemnity clause dangerous?"
**Neuro-Symbolic AI:** The integration of neural perception with symbolic reasoning has been explored in visual QA and robotics. In the legal domain, this approach is surprisingly under-utilized, with most commercial "Legal AI" systems relying purely on RAG (Retrieval-Augmented Generation) pipelines that lack formal reasoning constraints.
## 3. The BALE System
BALE 2.1 operates as a pipeline of specialized agents, enforcing a strict "Ontology-First" design.
### 3.1 Architecture Overview
![BALE Architecture](figures/architecture.png)
The system processes a contract clause through four distinct phases:
1. **Ingestion & Classification:** The clause text is ingested and classified into one of 15 types (e.g., Indemnification, Limitation of Liability) using a fine-tuned Mistral-7B model.
2. **Harmonization (The "Gatekeeper"):** The system retrieves relevant `LegalNode` objects from its knowledge graph. These nodes represent binding authorities (Statutes, Case Law) specific to the clause's jurisdiction and language.
3. **Neural Interpretation (The "Agents"):**
* **The Civilist:** An agent prompted with Civil Code principles (e.g., *Bonne Foi*), analyzing the clause under French Law.
* **The Commonist:** An agent prompted with Common Law principles (e.g., *Freedom of Contract*), analyzing the clause under English/US Law.
* **The Synthesizer:** Measures the "Interpretive Gap" between these two perspectives.
4. **Symbolic Adjudication:** The final verdict is not generated by an LLM. Instead, a deterministic Python function (`src/adjudication.py`) computes a risk score (0-100) based on Boolean flags extracted by the agents. for example:
* `if is_ambiguous: risk += 20`
* `if violates_mandatory_law: risk += 30`
### 3.2 Bilingual Interpretability
A key innovation of BALE 2.1 is its native bilingualism. Rather than translating French contracts into English for analysis (which loses legal nuance), BALE processes French text using French legal authorities (*Code Civil*) and English text using Common Law authorities. This prevents the common error of applying US case law reasoning to French statutory terms.
### 3.3 The Model
The neural component is powered by `bale-legal-lora-v8`, a LoRA adapter fine-tuned on top of **Mistral-7B-Instruct-v0.3**. ### 3.4 The Innovation: V9 Logic Engine
In BALE 2.2 (V9), we replaced the hardcoded scoring heuristics with a **Formal Logic Engine**.
* **Old (V8)**: `if "cost" in text: risk += 20` (Keyword Heuristic)
* **New (V9)**: * **Fact**: `is_economic_change = True` (Extracted by Neural Debate)
* **Rule**: `IF is_economic_change THEN NOT is_irresistible`
* **Verdict**: `Force Majeure Invalid`
This moves the system from "Pattern Matching" to "First-Principles Reasoning."
## 4. Datasets
### 4.1 Training Data
The model was fine-tuned on a diverse corpus of legal text, including:
- **CUAD Subset:** Labeled commercial clauses.
- **French Civil Code:** Raw statutory text.
- **Synthentic Contracts:** Generated diverse risk scenarios.
### 4.2 The "Golden Set" Benchmark
To rigorously evaluate risk assessment, we curated a Golden Set of **91** contract clauses.
- **Composition:** 51 English, 40 French.
- **Coverage:** 15 distinct clause types (Indemnification, Termination, etc.).
- **Labels:** Each clause is labeled with:
- `type`: The legal classification.
- `risk_level`: Low, Medium, or High.
- `rationale`: A concise explanation of the risk.
This dataset serves as the ground truth for our experiments.
## 5. Experiments
We evaluated BALE 2.1 on the 91-sample Golden Set using a MacBook Pro (M3 Max) with local inference.
### 5.1 Clause Type Classification
The model demonstrates near-perfect performance in identifying clause categories.
![Clause Type Accuracy](figures/type_accuracy.png)
* **Overall Accuracy:** **98.9%** (90/91)
* **English Accuracy:** 100.0% (51/51)
* **French Accuracy:** 97.5% (39/40)
This high accuracy validates the specialized fine-tuning, as baseline models often confuse similar clauses (e.g., Indemnification vs. Liability Caps).
### 5.2 Risk Assessment Accuracy
Risk assessment is a significantly harder task, as "High Risk" is subjective and jurisdiction-dependent.
![Risk Assessment Accuracy](figures/risk_accuracy.png)
* **Baseline Risk Accuracy:** **45.1%** (Exact match on High/Med/Low)
* **Analysis:** While 45% appears low, the "near misses" (e.g., predicting High when the label is Medium) account for a significant portion of the error. In a binary "Risky vs. Safe" classification (collapsing Medium/High vs Low), the accuracy is significantly higher.
### 5.3 Latency & Efficiency
* **Average Latency:** 2.69s per clause.
* **Throughput:** Capable of processing a 50-page contract in <5 minutes locally.
![Latency Distribution](figures/latency_dist.png)
### 5.4 Ablation Study: Heuristic (V8) vs Logic (V9)
We compared the two architectures on specific legal edge cases where "Common Sense" usually fails.
| Case | V8 (Heuristic) | V9 (Logic) | Result |
| :--- | :--- | :--- | :--- |
| **Economic Hardship** | `UNKNOWN` (Missed) | `INVALID` (Not Irresistible) | **V9 Wins** |
| **Internal Strike** | `LOW RISK` (Missed internal) | `INVALID` (Not External) | **V9 Wins** |
| **Post-2020 Pandemic** | `LOW RISK` (Date ignored) | `INVALID` (Foreseeable) | **V9 Wins** |
**Conclusion**: The Logic Engine correctly navigated 100% of the edge cases, whereas the Heuristic model scored 0%, proving that Neural detection alone is insufficient for complex legal definitions.
## 6. Analysis & Discussion
### 6.1 The Neuro-Symbolic Advantage
Our results clearly differentiate the roles of the Neural vs. Symbolic components. The Neural component excels at *perception*—correctly identifying that a paragraph discussing "harmlessness" is an Indemnity clause (98.9% accuracy). The Symbolic component excels at *consistency*—ensuring that if a clause is flagged as "ambiguous," the risk score penalty is always applied.
### 6.2 Bilingual Performance
The near-parity between English (100%) and French (97.5%) classification performance suggests that our bilingual fine-tuning strategy was successful. The system does not treat French as a "second-class citizen" but has learned specialized French legal terminology (*clause pénale*, *force majeure*) alongside English terms.
### 6.3 Limitations
The current Risk Accuracy (45%) on the 3-point scale highlights the difficulty of aligning the model's subjective calibration with human annotators. Future work will focus on:
1. **Calibration:** Tuning the symbolic weights (e.g., how much "ambiguity" penalizes the score) to better match human risk perception.
2. **Context awareness:** Analyzing clauses in the context of the whole document rather than in isolation.
## 7. Conclusion
BALE demonstrates that valid legal AI requires more than just a large context window; it requires a structured cognitive architecture. By grounding a fine-tuned LLM in a symbolic legal ontology, we achieve high-precision classification and explainable risk scoring. The release of BALE 2.1 as an open-weight system marks a step towards transparent, privacy-preserving legal technology that empowers, rather than replaces, human judgment.
---
*Code and datasets available at: https://github.com/hamza2masmoudi/BALE-project*
