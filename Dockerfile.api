# BALE API Production Dockerfile with V7 Model Support
# Supports both local MLX inference and external LLM endpoints

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    g++ \
    libpq-dev \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Install MLX for Apple Silicon inference (optional, works on supported hardware)
# Note: MLX only works on Apple Silicon, for other platforms use Mistral API
RUN pip install --no-cache-dir mlx mlx-lm || echo "MLX not available on this platform"

# Copy application code
COPY api/ ./api/
COPY src/ ./src/
COPY app/ ./app/
COPY database/ ./database/
COPY training/ ./training/
COPY alembic.ini .
COPY requirements.txt .

# Create necessary directories
RUN mkdir -p /app/data/chroma_db /app/models /app/logs

# V7 Model: Mount as volume for flexibility
# Volume: -v /path/to/models/bale-legal-lora-v7:/app/models/bale-legal-lora-v7
VOLUME ["/app/models", "/app/data"]

# Environment defaults
ENV PYTHONPATH=/app \
    PYTHONUNBUFFERED=1 \
    BALE_ENV=production \
    V7_ADAPTER_PATH=/app/models/bale-legal-lora-v7 \
    PORT=8000

# Expose ports
EXPOSE 8000 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

# Production: Use gunicorn with uvicorn workers
CMD ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
